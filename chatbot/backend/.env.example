# FastAPI Configuration
PORT=8000
ENVIRONMENT=development

# LiteLLM Configuration
# ---------------------
# LiteLLM provides a unified interface for multiple LLM providers.
# Simply set your API key and model, then LiteLLM handles the rest.

# Your LLM API Key (required)
API_KEY=your_api_key_here

# Model to use (required)
# Examples:
# - OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# - Anthropic: claude-3-opus-latest, claude-3-sonnet-20240229, claude-3-haiku-20240307
# - Google: gemini/gemini-pro, gemini/gemini-pro-vision
MODEL=gpt-4o-mini

# Optional: Override the provider (auto-detected from model name by default)
# PROVIDER=openai  # Options: openai, anthropic, google, azure, etc.

# Optional: Custom API base URL (for OpenAI-compatible endpoints like vLLM, LocalAI, etc.)
# API_BASE_URL=https://api.openai.com/v1  # Default for OpenAI
# API_BASE_URL=http://localhost:8000/v1   # Example for local vLLM with OpenAI compatibility

# Model Parameters (optional)
# MAX_TOKENS=1024
# TEMPERATURE=0.7