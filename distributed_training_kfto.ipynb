{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Fraud Detection model with the Kubeflow Training Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example fraud detection model is small and quickly trained. For many large models, training requires multiple GPUs and often multiple machines. In this notebook, you learn how to train a model by using the Kubeflow Training Operator on OpenShift AI to scale out model training. You use the Training Operator SDK to create a PyTorchJob that executes the provided model training script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Training Operator SDK\n",
    "\n",
    "The Training Operator SDK is not available by default with the Tensorflow workbench image. Run the following command to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqU kubeflow-training==1.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "Typically, the training data for your model is available in a shared location. For this example, the data is local. You upload it to your object storage so that you can learn how to load data from a shared data source. The provided model training script downloads the training data. The PyTorch DistributedSampler utility distributes the tasks among worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "\n",
    "import utils.s3\n",
    "\n",
    "utils.s3.upload_directory_to_s3(\"data\", \"data\")\n",
    "print(\"---\")\n",
    "utils.s3.list_objects(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate the Training Operator SDK to the OpenShift cluster\n",
    "\n",
    "The Training Operator SDK requires authenticated access to the OpenShift cluster so that it can create PyTorchJobs. The easiest way to get access details is by using the OpenShift web console. \n",
    " \n",
    "\n",
    "1. To generate the command, select **Copy login command** from the username drop-down menu at the top right of the OpenShift web console.\n",
    "\n",
    "    <figure>\n",
    "        <img src=\"./assets/copy-login.png\"  alt=\"copy login\"  >\n",
    "    <figure/>\n",
    "\n",
    "2. Click **Display token**.\n",
    "\n",
    "3. Below **Log in with this token**, take note of the parameters for token and server.\n",
    "   For example:\n",
    "    ```\n",
    "    oc login --token=sha256~LongString --server=https://api.your-cluster.domain.com:6443\n",
    "    ```    \n",
    "    - token: `sha256~LongString`\n",
    "    - server: `https://api.your-cluster.domain.com:6443`\n",
    "    \n",
    "4. In the following code cell, replace the token and server values with the values that you noted in Step 3.\n",
    "   For example:\n",
    "   ```\n",
    "   api_server = \"https://api.your-cluster.domain.com:6443\"\n",
    "   token = \"sha256~LongString\"\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "\n",
    "api_server = \"https://XXXX\"\n",
    "token = \"sha256~XXXX\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "#configuration.verify_ssl = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running the distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Training client\n",
    "\n",
    "Initialize the Training client by using the provided user credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "\n",
    "client = TrainingClient(client_configuration=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorchJob\n",
    "\n",
    "Use the Training Operator SDK client to submit a PyTorchJob.\n",
    "\n",
    "The model training script is imported from the `kfto-scripts` folder.\n",
    "\n",
    "The model training script loads and distributes the training data set among nodes, performs distributed training, evaluates by using the test data set, and exports the trained model to ONNX format and uploads it to the S3 bucket that is specified in the provided connection.\n",
    "\n",
    "*Important note:* If the following is true, you must edit the script in the next Python cell to uncomment the label declaration in the `create_job` function.\n",
    "\n",
    "* You are not using the Red Hat Sandbox test environment. \n",
    "\n",
    "* The Kueue component is enabled for OpenShift AI and you have created all Kueue related resources (`ResourceFlavor`, `ClusterQueue`, and `LocalQueue`) and set the `local_queue_name` to \"local-queue\", as described in the _Setting up Kueue resources_ section of this Fraud Detection workshop/tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"./kfto-scripts\")  # needed to make training function available in the notebook\n",
    "from train_pytorch_cpu import train_func\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector\n",
    ")\n",
    "\n",
    "# Job name serves as a unique identifier to retrieve job-related information by using the SDK\n",
    "job_name = \"fraud-detection\"\n",
    "\n",
    "# If the Kueue component is enabled, and you have created the Kueue-related resources (ResourceFlavor, ClusterQueue and LocalQueue), provide the LocalQueue name on the following line:\n",
    "local_queue_name = \"local-queue\"\n",
    "\n",
    "client.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    train_func=train_func,\n",
    "    num_workers=2,\n",
    "    num_procs_per_worker=\"1\",\n",
    "    resources_per_worker={\n",
    "        \"memory\": \"4Gi\",\n",
    "        \"cpu\": 1,\n",
    "    },\n",
    "    base_image=\"quay.io/modh/training:py311-cuda124-torch251\",\n",
    "    # If the Kueue component is enabled and you have created the Kueue-related resources (ResourceFlavor, ClusterQueue and LocalQueue), then uncomment the following line to add the queue-name label:\n",
    "    # labels={\"kueue.x-k8s.io/queue-name\": \"local-queue\"},\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"AWS_ACCESS_KEY_ID\", value=os.environ.get(\"AWS_ACCESS_KEY_ID\")),\n",
    "        V1EnvVar(name=\"AWS_S3_BUCKET\", value=os.environ.get(\"AWS_S3_BUCKET\")),\n",
    "        V1EnvVar(name=\"AWS_S3_ENDPOINT\", value=os.environ.get(\"AWS_S3_ENDPOINT\")),\n",
    "        V1EnvVar(name=\"AWS_SECRET_ACCESS_KEY\", value=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")),\n",
    "    ],\n",
    "    packages_to_install=[\n",
    "        \"s3fs\",\n",
    "        \"boto3\",\n",
    "        \"scikit-learn\",\n",
    "        \"onnx\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query important job information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Wait until the job finishes\n",
    "print(f\"PyTorchJob '{job_name}' is running.\", end='')\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_job_running(name=job_name):\n",
    "            print(\".\", end='')\n",
    "        elif client.is_job_succeeded(name=job_name):\n",
    "            print(\".\")\n",
    "            print([x.message for x in client.get_job_conditions(name=job_name) if x.type == \"Succeeded\"][0])\n",
    "            break\n",
    "        elif client.is_job_failed(name=job_name):\n",
    "            print(\".\")\n",
    "            print([x.message for x in client.get_job_conditions(name=job_name) if x.type == \"Failed\"][0])\n",
    "            break\n",
    "        else:\n",
    "            print(f\"PyTorchJob '{job_name}' status not available or no conditions found.\")\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting PyTorchJob status: {e}.\")\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the job logs\n",
    "print(client.get_job_logs(name=job_name)[0][\"fraud-detection-master-0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delete jobs\n",
    "\n",
    "After the PyTorchJob is finished, you can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.delete_job(name=job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
