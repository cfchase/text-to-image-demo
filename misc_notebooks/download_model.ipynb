{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HuggingFace Models to Local Storage\n",
    "\n",
    "This notebook demonstrates how to download model weights from HuggingFace Hub to local storage for later use. This is useful when you want to:\n",
    "- Pre-download models for offline use\n",
    "- Store models in a specific location for deployment\n",
    "- Cache models to avoid repeated downloads\n",
    "- Load models from disk in production environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries. We only need `huggingface_hub` for downloading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Function\n",
    "\n",
    "This function downloads all model files from a HuggingFace repository to a local directory. It uses `snapshot_download` which:\n",
    "- Downloads all files in the repository\n",
    "- Preserves the directory structure\n",
    "- Handles large files efficiently\n",
    "- Supports authentication for gated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_id: str, output_dir: str, use_auth_token: bool = False):\n",
    "    \"\"\"\n",
    "    Download model weights from HuggingFace Hub.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model ID (e.g., 'stabilityai/stable-diffusion-3.5-medium')\n",
    "        output_dir: Directory to save the model\n",
    "        use_auth_token: Whether to use HuggingFace authentication token\n",
    "    \n",
    "    Returns:\n",
    "        Path to the downloaded model directory\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading model: {model_id}\")\n",
    "    print(f\"Output directory: {output_path.absolute()}\")\n",
    "    \n",
    "    try:\n",
    "        # Download all model files\n",
    "        snapshot_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=str(output_path),\n",
    "            local_dir_use_symlinks=False,  # Store actual files, not symlinks\n",
    "            use_auth_token=use_auth_token\n",
    "        )\n",
    "        print(f\"✓ Successfully downloaded model to {snapshot_path}\")\n",
    "        \n",
    "        # List downloaded files\n",
    "        files = list(output_path.rglob(\"*\"))\n",
    "        file_list = [f for f in files if f.is_file()]\n",
    "        print(f\"\\nDownloaded {len(file_list)} files:\")\n",
    "        \n",
    "        # Show first 10 files with sizes\n",
    "        for f in sorted(file_list)[:10]:\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  - {f.relative_to(output_path)} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        if len(file_list) > 10:\n",
    "            print(f\"  ... and {len(file_list) - 10} more files\")\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_size_gb = sum(f.stat().st_size for f in file_list) / (1024**3)\n",
    "        print(f\"\\nTotal size: {total_size_gb:.2f} GB\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to download model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Download a Small Test Model\n",
    "\n",
    "Let's start with a small model for testing. The `segmind/tiny-sd` model is a compact version of Stable Diffusion that's great for quick tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small test model\n",
    "model_path = download_model(\n",
    "    model_id=\"segmind/tiny-sd\",\n",
    "    output_dir=\"../models/tiny-sd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Download Stable Diffusion 3.5 Medium\n",
    "\n",
    "For production use, you might want to download larger models like Stable Diffusion 3.5. Note that some models require authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Stable Diffusion 3.5 Medium\n",
    "# Uncomment the following lines to download (requires ~10GB disk space)\n",
    "\n",
    "# model_path = download_model(\n",
    "#     model_id=\"stabilityai/stable-diffusion-3.5-medium\",\n",
    "#     output_dir=\"../models/sd3.5-medium\",\n",
    "#     use_auth_token=True  # May require authentication\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Download Fine-tuned Red Hat Dog Model\n",
    "\n",
    "This is a custom fine-tuned Stable Diffusion 3 model trained on Red Hat dog images. It's useful for generating images with the Red Hat mascot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Red Hat Dog fine-tuned SD3 model\n",
    "# This model is fine-tuned for generating Red Hat mascot images\n",
    "\n",
    "model_path = download_model(\n",
    "    model_id=\"cfchase/redhat-dog-sd3\",\n",
    "    output_dir=\"../models/redhat-dog-sd3\"\n",
    ")\n",
    "\n",
    "# You can then use this model with:\n",
    "# pipeline = DiffusionPipeline.from_pretrained(\"./models/redhat-dog-sd3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication for Gated Models\n",
    "\n",
    "Some models on HuggingFace require authentication. Here's how to set up authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_auth():\n",
    "    \"\"\"\n",
    "    Check and setup HuggingFace authentication.\n",
    "    \"\"\"\n",
    "    # Check for token in environment\n",
    "    token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "    \n",
    "    if token:\n",
    "        print(\"✓ Found HuggingFace token in environment\")\n",
    "        return True\n",
    "    \n",
    "    # Check for token file\n",
    "    token_file = Path.home() / \".huggingface\" / \"token\"\n",
    "    if token_file.exists():\n",
    "        print(\"✓ Found HuggingFace token file\")\n",
    "        return True\n",
    "    \n",
    "    print(\"⚠️ No HuggingFace token found\")\n",
    "    print(\"To authenticate:\")\n",
    "    print(\"1. Set HF_TOKEN environment variable, or\")\n",
    "    print(\"2. Run: huggingface-cli login\")\n",
    "    return False\n",
    "\n",
    "# Check authentication status\n",
    "has_auth = setup_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models from Disk\n",
    "\n",
    "Once downloaded, you can load these models directly from disk without needing internet access. Here's how to use the downloaded models with Diffusers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading a downloaded model with Diffusers\n",
    "# Uncomment to test (requires diffusers and torch installed)\n",
    "\n",
    "# from diffusers import DiffusionPipeline\n",
    "# import torch\n",
    "#\n",
    "# # Load from local path instead of HuggingFace ID\n",
    "# pipeline = DiffusionPipeline.from_pretrained(\n",
    "#     \"./models/tiny-sd\",  # Local path to downloaded model\n",
    "#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "# )\n",
    "#\n",
    "# # Move to GPU if available\n",
    "# if torch.cuda.is_available():\n",
    "#     pipeline = pipeline.to(\"cuda\")\n",
    "#\n",
    "# print(\"Model loaded successfully from disk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Download Multiple Models\n",
    "\n",
    "You can easily download multiple models in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to download\n",
    "models_to_download = [\n",
    "    {\"id\": \"segmind/tiny-sd\", \"dir\": \"./models/tiny-sd\"},\n",
    "    # Add more models here as needed\n",
    "    # {\"id\": \"runwayml/stable-diffusion-v1-5\", \"dir\": \"./models/sd-v1.5\"},\n",
    "    # {\"id\": \"stabilityai/stable-diffusion-2-1\", \"dir\": \"./models/sd-v2.1\"},\n",
    "]\n",
    "\n",
    "# Download all models\n",
    "for model in models_to_download:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    try:\n",
    "        download_model(model[\"id\"], model[\"dir\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {model['id']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Downloaded Models\n",
    "\n",
    "If you need to remove downloaded models to free up space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def remove_model(model_dir: str):\n",
    "    \"\"\"\n",
    "    Remove a downloaded model directory.\n",
    "    \"\"\"\n",
    "    model_path = Path(model_dir)\n",
    "    if model_path.exists():\n",
    "        shutil.rmtree(model_path)\n",
    "        print(f\"✓ Removed {model_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Directory not found: {model_path}\")\n",
    "\n",
    "# Example: Remove a model (uncomment to use)\n",
    "# remove_model(\"./models/tiny-sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Best Practices\n",
    "\n",
    "1. **Storage Location**: Choose a location with sufficient disk space. Large models can be 5-20GB+.\n",
    "\n",
    "2. **Network**: Downloads can be large. Use a stable, fast internet connection.\n",
    "\n",
    "3. **Authentication**: Some models require accepting terms of use on HuggingFace before downloading.\n",
    "\n",
    "4. **Caching**: `snapshot_download` automatically caches downloads. Subsequent downloads of the same model will be faster.\n",
    "\n",
    "5. **Version Control**: Don't commit large model files to git. Add your models directory to `.gitignore`.\n",
    "\n",
    "6. **Deployment**: For production, consider using object storage (S3, GCS) or persistent volumes to store models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
