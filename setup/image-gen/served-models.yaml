---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    maxLoadingConcurrency: '2'
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/template-display-name: Triton runtime 24.01
    opendatahub.io/template-name: tritonserver-24.01-py3
    openshift.io/display-name: NVIDIA Triton
  name: nvidia-triton
  namespace: image-generation
  labels:
    name: tritonserver-24.01-py3
    opendatahub.io/dashboard: 'true'
spec:
  supportedModelFormats:
    - autoSelect: true
      name: keras
      version: '2'
    - autoSelect: true
      name: onnx
      version: '1'
    - autoSelect: true
      name: pytorch
      version: '1'
    - autoSelect: true
      name: tensorflow
      version: '1'
    - autoSelect: true
      name: tensorflow
      version: '2'
    - autoSelect: true
      name: tensorrt
      version: '7'
  builtInAdapter:
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 360000
    runtimeManagementPort: 8001
    serverType: triton
  multiModel: true
  containers:
    - args:
        - '-c'
        - mkdir -p /models/_triton_models; chmod 777 /models/_triton_models; exec tritonserver "--model-repository=/models/_triton_models" "--model-control-mode=explicit" "--strict-model-config=false" "--strict-readiness=false" "--allow-http=true" "--allow-sagemaker=false"
      command:
        - /bin/sh
      image: 'nvcr.io/nvidia/tritonserver:24.01-py3'
      livenessProbe:
        exec:
          command:
            - curl
            - '--fail'
            - '--silent'
            - '--show-error'
            - '--max-time'
            - '9'
            - 'http://localhost:8000/v2/health/live'
        initialDelaySeconds: 5
        periodSeconds: 30
        timeoutSeconds: 10
      name: triton
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
          nvidia.com/gpu: 1
        requests:
          cpu: '1'
          memory: 16Gi
          nvidia.com/gpu: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  protocolVersions:
    - grpc-v2
  grpcEndpoint: 'port:8085'
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
  replicas: 1
  tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
  grpcDataEndpoint: 'port:8001'

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: text_encoder
    serving.kserve.io/deploymentMode: ModelMesh
  name: textencoder
  labels:
    name: textencoder
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: '1'
      runtime: nvidia-triton
      storage:
        key: aws-connection-my-storage
        path: text-to-image/notebook-output/onnx-redhat-dog/text_encoder

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: unet
    serving.kserve.io/deploymentMode: ModelMesh
  name: unet
  labels:
    name: unet
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: '1'
      runtime: nvidia-triton
      storage:
        key: aws-connection-my-storage
        path: text-to-image/notebook-output/onnx-redhat-dog/unet

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: vae_decoder
    serving.kserve.io/deploymentMode: ModelMesh
  name: vaedecoder
  labels:
    name: vaedecoder
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: '1'
      runtime: nvidia-triton
      storage:
        key: aws-connection-my-storage
        path: text-to-image/notebook-output/onnx-redhat-dog/vae_decoder

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: vae_encoder
    serving.kserve.io/deploymentMode: ModelMesh
  name: vaeencoder
  labels:
    name: vaeencoder
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: '1'
      runtime: nvidia-triton
      storage:
        key: aws-connection-my-storage
        path: text-to-image/notebook-output/onnx-redhat-dog/vae_encoder
