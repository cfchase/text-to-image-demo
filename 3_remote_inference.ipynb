{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698a981a-d201-40c0-b5eb-a951369b5807",
   "metadata": {},
   "source": [
    "# gRPC Remote Inference\n",
    "\n",
    "Now that we've served the model using RHODS Model Serving, we can explore how to use those APIs as an exercise.  First, to use each model, we'll [deconstruct the stable diffusion pipeline](https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline#deconstruct-the-stable-diffusion-pipeline).  As shown in these examples of using [Huggingface pretrained weights](misc-notebooks/stable_diffusion_huggingface.ipynb), [custom pretrained weights](misc-notebooks/stable_diffusion_huggingface_finetuned.ipynb), [local weights](text-to-image-demo/misc-notebooks/stable_diffusion_local.ipynb), we're going to replace our inference with calls to gRPC and use the models in a very similar fashion.\n",
    "\n",
    "This notebook assumes you've deployed the 4 onnx models created in Notebook 2, either manually or by importing [serving/models.yaml](serving/models.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c6911-4a6f-49e4-8493-9bef5ab94c43",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "As we're going to be making gRPC requests, we'll need to use grpc libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9997c8-e938-4543-8aaf-1c3c0213b685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade accelerate transformers ftfy\n",
    "!pip install -q git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873099e-7252-4412-82d0-ab40b9301920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio==1.56.0 grpcio-tools==1.33.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b19f6-c219-467c-9483-f3194f9b3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep -e grpcio -e protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ed8ae-ba9b-4f76-abe7-a97b9a903961",
   "metadata": {},
   "source": [
    "### The gRPC Connection\n",
    "\n",
    "The models are deployed in the same namespace as part of ModelMesh.  We can find these settings in the RHODS UI.\n",
    "\n",
    "After specifying the locations, we load python libraries generated from the [Kserve proto](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53913b79-e384-490f-b09b-5c78865ee0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_host = 'modelmesh-serving'\n",
    "grpc_port = 8033\n",
    "\n",
    "textencoder_model_name = 'textencoder'\n",
    "unet_model_name = 'unet'\n",
    "vaeencoder_model_name = 'vaeencoder'\n",
    "vaedecoder_model_name = 'vaedecoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9c822-6317-4643-a1f4-9dfc4bcb043b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./serving')\n",
    "\n",
    "import grpc\n",
    "import serving.grpc_predict_v2_pb2 as grpc_predict_v2_pb2\n",
    "import serving.grpc_predict_v2_pb2_grpc as grpc_predict_v2_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel(f\"{grpc_host}:{grpc_port}\")\n",
    "stub = grpc_predict_v2_pb2_grpc.GRPCInferenceServiceStub(channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16c22b-ae83-43d3-9441-79a408ad148b",
   "metadata": {},
   "source": [
    "### Replace models with gRPC calls\n",
    "\n",
    "Now instead of loading the model in PyTorch, we can create functions to make remote calls with the same argument data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640daeb-18fa-4146-a8a8-cc904ee72fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# replace text decoder with grpc requests\n",
    "# vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\", use_safetensors=True)\n",
    "def vae_decoder_grpc_request(latent_sample):\n",
    "    inputs = []\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[0].name = \"latent_sample\"\n",
    "    inputs[0].datatype = \"FP32\"\n",
    "    inputs[0].shape.extend([1, 4, 64, 64])\n",
    "    arr = latent_sample.flatten()\n",
    "    inputs[0].contents.fp32_contents.extend(arr)\n",
    "\n",
    "    request = grpc_predict_v2_pb2.ModelInferRequest()\n",
    "    request.model_name = vaedecoder_model_name\n",
    "    request.inputs.extend(inputs)\n",
    "\n",
    "    response = stub.ModelInfer(request)\n",
    "    out_sample = np.frombuffer(response.raw_output_contents[0], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(out_sample.reshape([1, 3, 512, 512]))\n",
    "\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
    "\n",
    "\n",
    "# replace text encoder with grpc requests\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\", use_safetensors=False\n",
    "# )\n",
    "\n",
    "def textencoder_grpc_request(input_arr):\n",
    "    inputs = []\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[0].name = \"input_ids\"\n",
    "    inputs[0].datatype = \"INT32\"\n",
    "    inputs[0].shape.extend([1, 77])\n",
    "    arr = input_arr.flatten()\n",
    "    inputs[0].contents.int_contents.extend(arr)\n",
    "\n",
    "    request = grpc_predict_v2_pb2.ModelInferRequest()\n",
    "    request.model_name = textencoder_model_name\n",
    "    request.inputs.extend(inputs)\n",
    "\n",
    "    response = stub.ModelInfer(request)\n",
    "    text_embeddings = np.frombuffer(response.raw_output_contents[0], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(text_embeddings.reshape([-1, 77, 768]), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# unet = UNet2DConditionModel.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\", use_safetensors=False\n",
    "# )\n",
    "\n",
    "def unet_grpc_request(encoder_hidden_states, timestep, sample):\n",
    "    inputs = []\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[0].name = \"encoder_hidden_states\"\n",
    "    inputs[0].datatype = \"FP32\"\n",
    "    inputs[0].shape.extend([2, 77, 768])\n",
    "    arr = encoder_hidden_states.flatten()\n",
    "    inputs[0].contents.fp32_contents.extend(arr)\n",
    "\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[1].name = \"timestep\"\n",
    "    inputs[1].datatype = \"INT64\"\n",
    "    inputs[1].shape.extend([2, 1])\n",
    "    arr = timestep.flatten()\n",
    "    inputs[1].contents.int64_contents.extend(arr)\n",
    "\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[2].name = \"sample\"\n",
    "    inputs[2].datatype = \"FP32\"\n",
    "    inputs[2].shape.extend([2, 4, 64, 64])\n",
    "    arr = sample.flatten()\n",
    "    inputs[2].contents.fp32_contents.extend(arr)\n",
    "\n",
    "    request = grpc_predict_v2_pb2.ModelInferRequest()\n",
    "    request.model_name = unet_model_name\n",
    "    request.inputs.extend(inputs)\n",
    "\n",
    "    response = stub.ModelInfer(request)\n",
    "    out_sample = np.frombuffer(response.raw_output_contents[0], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(out_sample.reshape([-1, 4, 64, 64]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10364f-e303-4eea-ba59-60a6a5887a9d",
   "metadata": {},
   "source": [
    "We can still use the schedule, but now without an gpus which have been offloaded to the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aecd2a-4738-43a6-ac7a-ba470669e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "scheduler = DDIMScheduler.from_pretrained(\"cfchase/stable-diffusion-rhteddy\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9d0ff-d115-4fa8-a3f0-ea9b552da2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just use cpu and offload gpu requests to grpc server\n",
    "torch_device = \"cpu\"\n",
    "\n",
    "#replace inference with gRPC\n",
    "# vae.to(torch_device)\n",
    "# text_encoder.to(torch_device)\n",
    "# unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fa4d5-26ab-4657-abda-43c194071b0c",
   "metadata": {},
   "source": [
    "This time, let's put Teddy in a new location... the beach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad402268-6b21-4b10-8650-eb0bebe4b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photo of a rhteddy dog on the beach\"]\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 50  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "\n",
    "# Seed generator to create the inital latent noise\n",
    "# generator = torch.manual_seed(0)  # manual\n",
    "generator = torch.Generator()  # random\n",
    "\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136f04-b5e6-45ff-944f-aa7cf592bfe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_encoder_args = text_input.input_ids.to(torch_device)\n",
    "    \n",
    "    # replace text encoder with grpc requests\n",
    "    # text_embeddings = text_encoder(text_encoder_args)[0]\n",
    "    text_embeddings = textencoder_grpc_request(text_input.input_ids.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce80a1d-e7b1-4859-af7d-d76a79316a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# replace text encoder with grpc requests\n",
    "# uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings = textencoder_grpc_request(uncond_input.input_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f63bf-a8c6-4bf8-9f45-57ae483c4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd8529-060c-4ec2-800a-1d56ea6bd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "    #(batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    (batch_size, 4, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353b080-ac5f-4735-8960-29568506a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "    \n",
    "    # replace unet encoder with grpc requests\n",
    "\n",
    "    # predict the noise residual\n",
    "    # with torch.no_grad():\n",
    "        # noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # torch.tensor([t, t]) instead of t to workaround batch error on triton grpc\n",
    "    noise_pred = unet_grpc_request(text_embeddings, torch.tensor([t, t]), latent_model_input)\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b476cf8-9405-48c0-a76e-c5b21d9b198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "# replace vae decoder with grpc requests\n",
    "# with torch.no_grad():\n",
    "    # image = vae.decode(latents).sample   \n",
    "image = vae_decoder_grpc_request(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5c618-a1fe-4b4d-be3d-c2289f36cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f38633-0d6f-4f6c-9ecc-9b0d8ff96c1d",
   "metadata": {},
   "source": [
    "Did Teddy go to the beach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
